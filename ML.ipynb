{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNRQqKVPC/YjWYLOKzeEyTl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srishti-parulekar/studyML/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "uegsjm3V2JAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ai is the ability of a computer to mimic human intelligence.\n",
        "Machine learning enables computers to learn from data and make predictions.\n",
        "\n",
        "ML process:\n",
        "Data preprocesing (import data, clean the data and split into training and testing sets).\n",
        "Modeling: build and train the model, make predictions.\n",
        "Evaluation: calculate performance metrics.\n",
        "\n",
        "Types of learning:\n",
        "Supervised Learning:\n",
        "\n",
        "What is feature scaling?\n",
        "Feature scaling is always applied to columns, never to data inside the rows.\n",
        "Two main types:\n",
        "Normalisation and Standardization:\n",
        "\n",
        "With normalising a column: each entry X is modified to become X - Xmin / Max - Min;\n",
        "Each new column entry is now between the values 0 and 1.\n",
        "\n",
        "Standardization: X - u / p where u is the average and p is the standard deviation. All the values mostly between [-3,3]\n",
        "\n",
        "Feature scaling is necessary when grouping data values or clustering algorithms to check compatibility between values and the scale of the columns is drastically different so that the values can be normalised and one column wont overpower the other.\n",
        "\n",
        "Dependent variables are the varaibles that you want your model to predict while independent variables or features are the variables that are given to the model and the model uses them to make the prediction.\n",
        "\n",
        "We use the iloc function from pandas: stands for locate indexes. This allows us to split the columns or rows of the dataframe to create a new data frame.\n",
        "\n",
        "Missing values can be replaced using SImpleImputer.\n",
        "\n",
        "IMP: upper bound of range in python\n",
        "The fit() method helps in fitting the data into a model, transform() method helps in transforming the data into a form that is more suitable for the model.\n",
        "\n",
        "How do we encode categorical data?\n",
        "If we encode the company names of India, France and Germany as 0,1,2 it may work for our operations but the model may assume that the ordering has some significance which is not the case, so this cannot be done as it could cause misinterpreted correlations.\n",
        "\n",
        "We can do so by using binary vectors for the (independent variables)countries ( 1,0,0 and 0,1,0 and 0,0,1) and for binary values (0,1) for the (dependent) outcomes being yes and no.\n",
        "This is done using One Hot Encoding.\n",
        "Label encoding is being used for the binary value encoding of no and yes.\n",
        "OneHotEncoder after transforming doesnt return a np array so that needs to be taken care of separately.\n",
        "\n",
        "Do we perform feature scaling before or after splitting the data into training and testing sets? AFTER.\n",
        "The test set is supposed to be a brand new set. The feature scaling takes the mean and the standard deviation of all values including the test set, there would be information leakage on the test set.\n",
        "\n",
        "When to use standardisation and when to use normalisation?\n",
        "Normalisation is great when you have a normal distribution in most of your features.\n",
        "Standardisation is great always.\n",
        "\n",
        "Do we have to apply feature scaling to the dummy variables in the matrix of features? NO.\n",
        "Post standardisation all the values of the variables will be between -3 and 3. Since dummy variables have binary values of 0 and 1 already, they are already in range. Standardisation will make it worse since it will transform these values to be between -3 and 3 but we will lose the interpretation of these values that we need to train our model.\n",
        "\n",
        "Delimiters used when the csv file has its row values separated by something other than commas,\n",
        "Example if separated by ;\n",
        "dataset = pd.read_csv('winequality-red.csv', sep=';')\n",
        "\n",
        "Regression models (both linear and non-linear) are used for predicting a real value, like salary for example. If your independent variable is time, then you are forecasting future values, otherwise your model is predicting present but unknown values."
      ],
      "metadata": {
        "id": "BS0-aluBNPYS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slA9vkHYcyy2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# numpy allows us to work with arrays\n",
        "import matplotlib.pyplot as plt\n",
        "# allows us to plot graphs, importing the pyplot module in specific\n",
        "import pandas as pd\n",
        "# good to preprocess datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('Data.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "# taking all the rows, and all the columns except the last column which is the dependent variable.\n",
        "y = dataset.iloc[:, -1].values\n",
        "# takes all the rows and only the last column\n",
        "# X and y are numpy arrays necessarily so that the train() function can be applied on them."
      ],
      "metadata": {
        "id": "VZ4po8yReOA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nukhbai3hHDe",
        "outputId": "dc483d94-7227-41fa-ad39-b6ad56dc5eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfvgFCPIhJdo",
        "outputId": "cfdb9c65-f1ca-4c4e-844d-17bbe4ed34f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking care of missing data!"
      ],
      "metadata": {
        "id": "dcBIVw_Sh-BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for large datasets, you can just delete the rows that have missing data.\n",
        "# not always efficient, can instead replace the missing data value.\n",
        "# missing value can be replaced by average or by the median value of that column or something else too.\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "#\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "# creating imputer object with parameters stating which data values to replace with what strategy.\n",
        "imputer.fit(X[:, 1:3])\n",
        "# since in python upperbound is not included for column number 1 and 2: [1,3] is used.\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])"
      ],
      "metadata": {
        "id": "jsNPRNsrh_zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQoh4qAFrHaP",
        "outputId": "c696edf5-e55f-42f4-d35b-f246db481102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Hot Encoding\n"
      ],
      "metadata": {
        "id": "hBZNNlVmQqdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the independent variable:\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "lPs-aPpnQsLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "# transformer parameter contains the transformation that needs to be done: encoding, the class that will perform the encoding\n",
        "# ie OneHotEncoder, the third element is the indexes of the column on which we want to perform the encoding,\n",
        "# here it is the country column only.\n",
        "\n",
        "# remainder is going to state that we want to keep the columns that we are not applying the transformation too as well.\n",
        "\n",
        "X = np.array(ct.fit_transform(X))\n",
        "# here ct.fit_transform() doesnt automatically return a numpy array"
      ],
      "metadata": {
        "id": "wh6NgZOaQ7IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLHIHDiqTfzg",
        "outputId": "fd93dc7f-228c-4422-8c84-96921a85a67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "france is encoded as 1.0, 0.0, 0.0\n",
        "germany as 0.0 0.0 1.0 and so on"
      ],
      "metadata": {
        "id": "pqM_VujrT0Yz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the dependent var"
      ],
      "metadata": {
        "id": "vHFqKq9VUF_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "2mHfEd-BT8WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "vEbW7dvEUWR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9vjKRaJUgdd",
        "outputId": "3b9bedef-6b6a-4b9c-d166-19d8ef54ed69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset into training and testing."
      ],
      "metadata": {
        "id": "CDrD1g5wX1Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=1)\n",
        "\n",
        "# ensures that the same randomization is used each time you run the code, resulting in the same splits of the data."
      ],
      "metadata": {
        "id": "ykkKzB5HX4KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPBizhnHoPWq",
        "outputId": "71b674c9-efbf-40dc-a40f-afebe8fb589c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQVUwIbWoSED",
        "outputId": "af9a6817-8e33-452f-8c3a-cd8e7f88a948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOHr-QTyoT_y",
        "outputId": "83136932-d15c-436d-c28f-03755c082b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4whLqLYHoVki",
        "outputId": "bee19097-9ac6-4564-a9cb-f49f7d2421ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scaling (always after Splitting!)"
      ],
      "metadata": {
        "id": "qeI_BW7loglg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# not always applied\n",
        "# normalisation good when normal distribution for most of your features, standardisation is always good.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n"
      ],
      "metadata": {
        "id": "dinnQ_bQokhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[:,3:] = sc.fit_transform(X_train[:,3:])\n",
        "# fit will just give the mean and the std dev of the columns, transform will actually change each entry in the column.\n",
        "# by applying the formula."
      ],
      "metadata": {
        "id": "VdzNPXR7wC5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to standardise the test values by the same scaler (ie using the calculated mean and std dev of the training set)\n",
        "# and not its own scaler since that would cause information leakage,\n",
        "# we already fitted sc with the scaler for the training set so we use the same values of mean and std dev\n",
        "# and directly apply transform of the test set.\n",
        "X_test[:,3:] = sc.transform(X_test[:,3:])"
      ],
      "metadata": {
        "id": "HvwoVI-4wXdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urwrcw4CxB1h",
        "outputId": "fd93aca9-9b38-4d14-e555-cc43cbcdd613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jcXYRicxE4U",
        "outputId": "a65a8b1d-c3ab-43c5-94b2-6bbd085b81f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Regression"
      ],
      "metadata": {
        "id": "isuoEkxR2DEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression models (both linear and non-linear) are used for predicting a real value, like salary for example.\n",
        "# If your independent variable is time, then you are forecasting future values, otherwise your model is predicting present but unknown values."
      ],
      "metadata": {
        "id": "_0_7WXKoKsSP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}